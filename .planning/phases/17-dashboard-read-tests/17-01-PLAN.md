---
phase: 17-dashboard-read-tests
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/e2e/src/fixtures/mock-project.ts
  - packages/e2e/src/dashboard.test.ts
autonomous: true
requirements:
  - DASH-01
  - DASH-02
  - DASH-03
  - DASH-04
  - DASH-05

must_haves:
  truths:
    - "Dashboard server spawns from installed path, boots cleanly, and /api/health returns { status: 'ok' } within 30s — no fixed delays"
    - "/api/project response body contains the string 'Mock Test Project' and 'Validates maxsim-tools'"
    - "/api/phases response is an array with at least 1 entry whose name includes 'foundation'"
    - "/api/state response body.decisions is an array containing an entry that includes 'Mock decision one'"
    - "/api/todos response body.pending is an array containing at least one entry with text 'Test Task'"
  artifacts:
    - path: "packages/e2e/src/fixtures/mock-project.ts"
      provides: "Mock project fixture with corrected todo title frontmatter and 02-integration phase directory"
      contains: "title: Test Task"
    - path: "packages/e2e/src/dashboard.test.ts"
      provides: "E2E dashboard read API test suite"
      exports: ["describe('dashboard read API')"]
      min_lines: 80
  key_links:
    - from: "packages/e2e/src/dashboard.test.ts"
      to: "{installDir}/.claude/dashboard/server.js"
      via: "child_process.spawn in beforeAll"
      pattern: "spawn.*server\\.js"
    - from: "dashboard.test.ts beforeAll"
      to: "baseUrl"
      via: "stderr parsing of 'Dashboard ready at http://localhost:{port}'"
      pattern: "Dashboard ready at.*localhost"
    - from: "dashboard.test.ts it blocks"
      to: "baseUrl + /api/*"
      via: "global fetch calls"
      pattern: "fetch.*baseUrl.*api"
---

<objective>
Fix the mock project fixture to align with dashboard parser behavior, then write the full dashboard.test.ts E2E test file that spawns the dashboard server and validates all five read-only API endpoints.

Purpose: Proves that the dashboard server ships correctly in the npm tarball and its API reads project data from MAXSIM_PROJECT_CWD — the core requirement for dashboard npm delivery validation.
Output: A passing `nx run e2e:e2e` run that includes dashboard server spawn, health polling, and five endpoint assertions.
</objective>

<execution_context>
@./workflows/execute-plan.md
@./templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@packages/e2e/src/fixtures/mock-project.ts
@packages/e2e/src/globalSetup.ts
@packages/e2e/src/install.test.ts
@packages/e2e/src/vitest.d.ts
@packages/e2e/vitest.config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix mock-project.ts fixture for dashboard API compatibility</name>
  <files>packages/e2e/src/fixtures/mock-project.ts</files>
  <action>
Two corrections are required before dashboard test assertions can pass:

**Correction 1 — Todo title frontmatter (DASH-05 fix):**
The dashboard `parseTodos()` function extracts todo text via `content.match(/^title:\s*(.+)$/m)`. The current mock todo file starts with `# Todo: Test Task` but has no `title:` frontmatter. Without `title:`, the parser falls back to the filename (`"todo-001-test-task"`), not `"Test Task"`.

In the `createMockProject()` function, update the pending todo file content to prepend `title: Test Task` as the very first line before the `#` heading:

```
title: Test Task
# Todo: Test Task

**Area:** general

Do the test task for E2E validation.
```

**Correction 2 — Second phase directory (DASH-03 fix):**
The dashboard `parsePhases()` scans `.planning/phases/` directories. The mock creates only `01-foundation/`. The CONTEXT.md assertion expects `body.length === 2` with both "Foundation" and "Integration". Add a second phase directory `02-integration` to make the mock match the ROADMAP.md entries.

In `createMockProject()`, add this directory creation after the existing `mkdirSync` calls:
```ts
mkdirSync(join(dir, '.planning', 'phases', '02-integration'), { recursive: true });
```

No other changes to this file. Do NOT change the interface, the cleanup logic, the other writeFileSync calls, or the return value.
  </action>
  <verify>
    <automated>node -e "const {createMockProject} = require('./packages/e2e/src/fixtures/mock-project.ts'); console.log('import ok')" 2>&amp;1 || npx tsx -e "import { createMockProject } from './packages/e2e/src/fixtures/mock-project.ts'; const p = createMockProject(); const fs = await import('node:fs'); const todo = fs.readFileSync(p.dir + '/.planning/todos/pending/todo-001-test-task.md', 'utf8'); console.log(todo.startsWith('title: Test Task') ? 'PASS' : 'FAIL: missing title frontmatter'); const dirs = fs.readdirSync(p.dir + '/.planning/phases'); console.log(dirs.includes('02-integration') ? 'PASS' : 'FAIL: missing 02-integration dir'); p.cleanup();"</automated>
  </verify>
  <done>
    - `createMockProject()` todo file starts with `title: Test Task\n` as line 1
    - `createMockProject()` creates both `01-foundation/` and `02-integration/` in `.planning/phases/`
    - No other mock fixture behavior changed
  </done>
</task>

<task type="auto">
  <name>Task 2: Write dashboard.test.ts E2E dashboard read API tests</name>
  <files>packages/e2e/src/dashboard.test.ts</files>
  <action>
Create `packages/e2e/src/dashboard.test.ts` implementing the full dashboard read API test suite.

**CRITICAL implementation details from research:**

1. **Port discovery via stderr** — The server.js calls `detectPort(3333)` internally and does NOT read `process.env.PORT`. Do NOT set PORT in spawn env and do NOT poll a pre-chosen port. Instead, capture the stderr stream and extract the port from the log line `"Dashboard ready at http://localhost:{port}"`.

2. **Two-phase startup** — After extracting the port URL from stderr, still call `pollUntilReady(baseUrl)` to confirm Next.js is fully ready (Next.js may still be initializing when the server logs the URL).

3. **`/api/phases` length** — After Task 1's fixture fix, the mock has two phase directories (`01-foundation/` and `02-integration/`). Assert `body.length === 2` and check names include both "foundation" and "integration".

4. **`/api/todos` text** — After Task 1's fixture fix, the todo file has `title: Test Task` frontmatter. Assert `body.pending.some(t => t.text === 'Test Task')`.

5. **`vitest.d.ts`** — Do NOT modify. The `installDir` inject is already declared. Compute the dashboard server path inline from `inject('installDir')`.

6. **`hookTimeout`** — vitest.config.ts already sets `hookTimeout: 120_000`. The `beforeAll` must be given an explicit `35_000` ms timeout as the second argument to account for Next.js cold start and give a clear Vitest timeout error vs. the inner 30s Promise timeout.

**Full file to create:**

```typescript
import { describe, it, expect, beforeAll, afterAll } from 'vitest';
import { inject } from 'vitest';
import { spawn, type ChildProcess } from 'node:child_process';
import { join } from 'node:path';
import { createMockProject, type MockProject } from './fixtures/mock-project.js';

let server: ChildProcess | null = null;
let baseUrl = '';
let mockProject: MockProject | null = null;

async function pollUntilReady(url: string, timeoutMs = 30_000): Promise<void> {
  const deadline = Date.now() + timeoutMs;
  while (Date.now() < deadline) {
    try {
      const res = await fetch(`${url}/api/health`, {
        signal: AbortSignal.timeout(500),
      });
      if (res.ok) {
        const body = await res.json() as { status: string };
        if (body.status === 'ok') return;
      }
    } catch {
      // Not ready yet — retry
    }
    await new Promise(resolve => setTimeout(resolve, 250));
  }
  throw new Error(`Server at ${url} did not become healthy within ${timeoutMs}ms`);
}

describe('dashboard read API', () => {
  beforeAll(async () => {
    mockProject = createMockProject();
    const installDir = inject('installDir');
    const serverPath = join(installDir, '.claude', 'dashboard', 'server.js');
    const serverDir = join(installDir, '.claude', 'dashboard');

    server = spawn('node', [serverPath], {
      cwd: serverDir,
      stdio: ['ignore', 'pipe', 'pipe'],
      env: {
        ...process.env,
        MAXSIM_PROJECT_CWD: mockProject.dir,
        NODE_ENV: 'production',
      },
    });

    // Step 1: Discover actual port from stderr — server owns port via detectPort(3333)
    baseUrl = await new Promise<string>((resolve, reject) => {
      const timeout = setTimeout(
        () => reject(new Error('Dashboard did not log startup URL within 30s')),
        30_000
      );

      server!.stderr!.on('data', (chunk: Buffer) => {
        const text = chunk.toString();
        const match = text.match(/Dashboard ready at (http:\/\/localhost:\d+)/);
        if (match) {
          clearTimeout(timeout);
          resolve(match[1]);
        }
      });

      server!.on('error', (err: Error) => {
        clearTimeout(timeout);
        reject(err);
      });

      server!.on('exit', (code: number | null) => {
        if (code !== 0 && code !== null) {
          clearTimeout(timeout);
          reject(new Error(`Dashboard server exited with code ${code}`));
        }
      });
    });

    // Step 2: Poll /api/health to confirm Next.js is fully ready for requests
    await pollUntilReady(baseUrl, 10_000);
  }, 35_000);

  afterAll(async () => {
    if (server) {
      await new Promise<void>((resolve) => {
        server!.on('close', () => resolve());
        server!.kill('SIGTERM');
        // Force resolve after 5s to avoid hanging afterAll
        setTimeout(resolve, 5_000);
      });
      server = null;
    }
    mockProject?.cleanup();
    mockProject = null;
  });

  it('DASH-01: /api/health returns { status: ok }', async () => {
    const res = await fetch(`${baseUrl}/api/health`);
    expect(res.status).toBe(200);
    const body = await res.json() as { status: string };
    expect(body.status).toBe('ok');
  });

  it('DASH-02: /api/project contains mock project name and core value', async () => {
    const res = await fetch(`${baseUrl}/api/project`);
    expect(res.status).toBe(200);
    const body = await res.json() as { project: string | null; requirements: string | null };
    expect(body.project).toContain('Mock Test Project');
    expect(body.project).toContain('Validates maxsim-tools');
  });

  it('DASH-03: /api/phases returns array with foundation and integration phases', async () => {
    const res = await fetch(`${baseUrl}/api/phases`);
    expect(res.status).toBe(200);
    const body = await res.json() as Array<{ name: string }>;
    expect(Array.isArray(body)).toBe(true);
    expect(body.length).toBe(2);
    const names = body.map(p => p.name.toLowerCase());
    expect(names.some(n => n.includes('foundation'))).toBe(true);
    expect(names.some(n => n.includes('integration'))).toBe(true);
  });

  it('DASH-04: /api/state decisions array contains Mock decision one', async () => {
    const res = await fetch(`${baseUrl}/api/state`);
    expect(res.status).toBe(200);
    const body = await res.json() as { decisions: string[] };
    expect(Array.isArray(body.decisions)).toBe(true);
    expect(body.decisions.some(d => d.includes('Mock decision one'))).toBe(true);
  });

  it('DASH-05: /api/todos pending array contains Test Task', async () => {
    const res = await fetch(`${baseUrl}/api/todos`);
    expect(res.status).toBe(200);
    const body = await res.json() as { pending: Array<{ text: string }> };
    expect(Array.isArray(body.pending)).toBe(true);
    expect(body.pending.some(t => t.text === 'Test Task')).toBe(true);
  });
});
```

Write this file exactly as shown. The `.js` extension on the mock-project import is required for ESM resolution in the Vitest runner.
  </action>
  <verify>
    <automated>npx tsc --noEmit --project packages/e2e/tsconfig.json 2>&amp;1 | head -20 || echo "TypeCheck done"</automated>
  </verify>
  <done>
    - `packages/e2e/src/dashboard.test.ts` exists and contains all five `it()` blocks (DASH-01 through DASH-05)
    - `pollUntilReady` function defined in the file (not imported)
    - `beforeAll` uses stderr parsing to discover port — does NOT use PORT env var
    - `afterAll` kills server with SIGTERM, waits for close event, then cleans up mock project
    - TypeScript compilation reports no errors in the file
    - `nx run e2e:e2e` passes (requires pre-built dashboard: `STANDALONE_BUILD=true nx build dashboard && nx build cli` first)
  </done>
</task>

</tasks>

<verification>
The full E2E suite (`nx run e2e:e2e`) includes dashboard read tests. Prerequisite: `STANDALONE_BUILD=true nx build dashboard && nx build cli` must run first so `dist/assets/dashboard/server.js` exists.

Run sequence:
1. `STANDALONE_BUILD=true nx build dashboard` — builds Next.js standalone output
2. `nx build cli` — copies dashboard into `dist/assets/dashboard/` via copy-assets.cjs
3. `nx run e2e:e2e` — pack + install + dashboard tests

All five DASH-* requirements satisfied when:
- Server spawns and health check passes within 35s
- Five API endpoint assertions all pass
- No server process leaks (afterAll kills cleanly)
</verification>

<success_criteria>
- `packages/e2e/src/fixtures/mock-project.ts` has `title: Test Task` on line 1 of todo file content and creates `02-integration/` phase directory
- `packages/e2e/src/dashboard.test.ts` exists with 5 passing it() blocks
- `nx run e2e:e2e` exits 0 (after pre-building dashboard)
- No TypeScript errors in dashboard.test.ts
- DASH-01, DASH-02, DASH-03, DASH-04, DASH-05 all covered
</success_criteria>

<output>
After completion, create `.planning/phases/17-dashboard-read-tests/17-01-SUMMARY.md`
</output>
